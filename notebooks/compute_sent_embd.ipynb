{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Reports, Extract Word Embeddings, and Compute Sentence Embeddings\n",
    "\n",
    "This notebook prepares and processes narrative clinical reports for downstream survival analysis. It includes the following key steps:\n",
    "\n",
    "1. **Loading and Preprocessing Raw Reports**  \n",
    "   Raw patient-level data is imported from CSV using `import_and_prepare_dataframe`, which handles target encoding, date parsing, and computation of per-patient start and end dates.\n",
    "\n",
    "2. **Loading the Pretrained NLP Model**  \n",
    "   A BERT-based model and its tokenizer are loaded with `load_nlp_model`, and assigned to the appropriate computation device (CPU or GPU).\n",
    "\n",
    "3. **Sentence Embedding Computation**  \n",
    "   Sentence embeddings are computed from text using `compute_sentence_embeddings`, with support for both CLS token and SIF-based methods.\n",
    "\n",
    "4. **Batch Processing and Export**  \n",
    "   Using `process_and_export_embeddings`, the full dataset is split into several parts, sentence embeddings are computed, unused columns are removed, and each batch is exported to a CSV file for efficient downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import sys\n",
    "from numbers import Real, Integral\n",
    "\n",
    "# Create a fake module to emulate 'sklearn.utils._param_validation'\n",
    "# (used by skglm in newer versions of scikit-learn, >=1.3)\n",
    "param_validation = types.ModuleType(\"sklearn.utils._param_validation\")\n",
    "\n",
    "# Define a minimal replacement for Interval used in _parameter_constraints\n",
    "class Interval:\n",
    "    def __init__(self, dtype, left, right, closed=\"neither\"):\n",
    "        self.dtype = dtype\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.closed = closed\n",
    "\n",
    "# Define a minimal replacement for StrOptions used in _parameter_constraints\n",
    "class StrOptions:\n",
    "    def __init__(self, options):\n",
    "        self.options = set(options)\n",
    "\n",
    "# Add the custom classes to the fake module\n",
    "param_validation.Interval = Interval\n",
    "param_validation.StrOptions = StrOptions\n",
    "\n",
    "# Inject the fake module into sys.modules before skglm is imported\n",
    "# This prevents skglm from raising an ImportError if sklearn < 1.3\n",
    "sys.modules[\"sklearn.utils._param_validation\"] = param_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/sigbert-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "src_path = os.path.abspath(os.path.join(notebook_dir, '..', 'src/sigbert'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Now import our custom modules\n",
    "from _utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at ../models/OncoBERT_v1.0 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "tokenizer, model, device = load_nlp_model(path_model=\"../models/OncoBERT_v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to input reports\n",
    "path_import = ...\n",
    "df = import_and_prepare_dataframe(path_import)\n",
    "\n",
    "# Define export path\n",
    "export_path = ...\n",
    "print(\"export_path = \", export_path)\n",
    "\n",
    "cols_to_drop = ['text', 'word_embeddings', 'embeddings']\n",
    "\n",
    "# Process and export the dataset with sentence embeddings\n",
    "df_short_fin = process_and_export_embeddings(\n",
    "    df, tokenizer, model, device, export_path, \n",
    "    method_embd=\"Arora\", cols_to_drop=cols_to_drop\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "vscode": {
   "interpreter": {
    "hash": "daf280de155aa79ff4dc003da54ee1ffa56017c0e371c9885c08a7947b1313a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
